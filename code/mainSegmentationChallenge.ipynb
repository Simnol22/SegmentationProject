{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b07cf2f-b81d-43ef-907a-c49fed2d95ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Please cite the following paper when using nnUNet:\n",
      "\n",
      "Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n",
      "\n",
      "\n",
      "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from progressBar import printProgressBar\n",
    "\n",
    "from torchgeometry.losses import tversky\n",
    "\n",
    "import medicalDataLoader\n",
    "import argparse\n",
    "from utils import *\n",
    "\n",
    "from nnUnet import *\n",
    "from UNet_Base import *\n",
    "import random\n",
    "import torch\n",
    "import pdb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torchmetrics import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verifify if you are using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is not available\n"
     ]
    }
   ],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    print(\"Cuda is not available\")\n",
    "else:\n",
    "    nbDevices = torch.cuda.device_count()\n",
    "    print(\"Cuda is available, with \", nbDevices, \" devices\")\n",
    "    if nbDevices:\n",
    "        print(\"running on : \", torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e976350-3fd5-4406-8511-86a06a9b4494",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        # print(inputs.shape())\n",
    "        # print(targets.shape())\n",
    "        inputs = F.sigmoid(inputs)\n",
    "\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n",
    "\n",
    "        return 1 - dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "220c7dcc-8438-454d-97b1-8e989a7b8f06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def runTraining(modelName,checkpoints=None, n_epochs=0):\n",
    "    print('-' * 40)\n",
    "    print('~~~~~~~~  Starting the training... ~~~~~~')\n",
    "    print('-' * 40)\n",
    "\n",
    "    ## DEFINE HYPERPARAMETERS (batch_size > 1)\n",
    "    batch_size = 16\n",
    "    batch_size_val = 8\n",
    "    lr = 0.001\n",
    "    epoch = 10\n",
    "    start_epoch = 0\n",
    "    if checkpoints != None :\n",
    "        batch_size = checkpoints['batch_size']\n",
    "        batch_size_val = checkpoints['batch_size_val']\n",
    "        lr = checkpoints['lr']    # Learning Rate\n",
    "        epoch = n_epochs # Number of epochs\n",
    "        start_epoch = checkpoints['epoch']\n",
    "        \n",
    "\n",
    "    root_dir = './Data/'\n",
    "\n",
    "    print(' Dataset: {} '.format(root_dir))\n",
    "\n",
    "    ## DEFINE THE TRANSFORMATIONS TO DO AND THE VARIABLES FOR TRAINING AND VALIDATION\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    mask_transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    train_set_full = medicalDataLoader.MedicalImageDataset('train',\n",
    "                                                      root_dir,\n",
    "                                                      transform=transform,\n",
    "                                                      mask_transform=mask_transform,\n",
    "                                                      augment=False,\n",
    "                                                      equalize=False)\n",
    "\n",
    "    train_loader_full = DataLoader(train_set_full,\n",
    "                              batch_size=batch_size,\n",
    "                              worker_init_fn=np.random.seed(0),\n",
    "                              num_workers=0,\n",
    "                              shuffle=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    val_set = medicalDataLoader.MedicalImageDataset('val',\n",
    "                                                    root_dir,\n",
    "                                                    transform=transform,\n",
    "                                                    mask_transform=mask_transform,\n",
    "                                                    equalize=False)\n",
    "\n",
    "    val_loader = DataLoader(val_set,\n",
    "                            batch_size=batch_size_val,\n",
    "                            worker_init_fn=np.random.seed(0),\n",
    "                            num_workers=0,\n",
    "                            shuffle=False)\n",
    "\n",
    "\n",
    "    ## INITIALIZE YOUR MODEL\n",
    "    num_classes = 4 # NUMBER OF CLASSES\n",
    "\n",
    "    print(\"~~~~~~~~~~~ Creating the UNet model ~~~~~~~~~~\")\n",
    "    modelName = 'Test_Model'\n",
    "    print(\" Model Name: {}\".format(modelName))\n",
    "\n",
    "    ## CREATION OF YOUR MODEL\n",
    "    net = UNet(num_classes)\n",
    "    if checkpoints != None and checkpoints['model_state_dict'] != None:\n",
    "        net.load_state_dict = checkpoints['model_state_dict']\n",
    "    \n",
    "\n",
    "    print(\"Total params: {0:,}\".format(sum(p.numel() for p in net.parameters() if p.requires_grad)))\n",
    "\n",
    "    # DEFINE YOUR OUTPUT COMPONENTS (e.g., SOFTMAX, LOSS FUNCTION, ETC)\n",
    "    softMax = torch.nn.Softmax()\n",
    "    CE_loss = torch.nn.CrossEntropyLoss()\n",
    "    # CE_loss = torch.nn.FocalLoss(weight=net.parameters(), ignore_index=255,\n",
    "    #                              size_average=True)\n",
    "\n",
    "    ## PUT EVERYTHING IN GPU RESOURCES    \n",
    "    if torch.cuda.is_available():\n",
    "        net.cuda()\n",
    "        softMax.cuda()\n",
    "        CE_loss.cuda()\n",
    "\n",
    "    ## DEFINE YOUR OPTIMIZER\n",
    "    # optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=5e-4,momentum=0.9)\n",
    "\n",
    "    if checkpoints != None and checkpoints['optimizer_state_dict'] != None:\n",
    "        optimizer.load_state_dict = checkpoints['optimizer_state_dict']\n",
    "\n",
    "    ### To save statistics ####\n",
    "    lossTotalTraining = []\n",
    "    Best_loss_val = 1000\n",
    "    if checkpoints != None :\n",
    "        Best_loss_val = checkpoints['val_loss']\n",
    "    BestEpoch = 0\n",
    "    \n",
    "    directory = 'Results/Statistics/' + modelName\n",
    "\n",
    "    print(\"~~~~~~~~~~~ Starting the training ~~~~~~~~~~\")\n",
    "    if os.path.exists(directory)==False:\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    ## START THE TRAINING\n",
    "    \n",
    "    ## FOR EACH EPOCH\n",
    "    for i in range(start_epoch, epoch):\n",
    "        net.train()\n",
    "        lossEpoch = []\n",
    "        lossValEpoch = []\n",
    "        mean_acc = np.array([0,0,0,0]).astype(float)\n",
    "        mean_val_acc = np.array([0,0,0,0]).astype(float)\n",
    "        DSCEpoch = []\n",
    "        DSCEpoch_w = []\n",
    "        num_batches = len(train_loader_full)\n",
    "        num_batches_val = len(val_loader)\n",
    "        \n",
    "        n = 0\n",
    "        ## FOR EACH BATCH\n",
    "        for j, data in enumerate(train_loader_full):\n",
    "            ### Set to zero all the gradients\n",
    "            net.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ## GET IMAGES, LABELS and IMG NAMES\n",
    "            images, labels, _ = data\n",
    "\n",
    "            ### From numpy to torch variables\n",
    "            labels = to_var(labels)\n",
    "            # labels = torch.argmax(labels, dim=1)\n",
    "            images = to_var(images)\n",
    "\n",
    "            ################### Train ###################\n",
    "            #-- The CNN makes its predictions (forward pass)\n",
    "            net_predictions = net.forward(images)\n",
    "            print(net_predictions)\n",
    "            #-- Compute the losses --#\n",
    "            # THIS FUNCTION IS TO CONVERT LABELS TO A FORMAT TO BE USED IN THIS CODE\n",
    "            segmentation_classes = getTargetSegmentation(labels)\n",
    "            \n",
    "            # COMPUTE THE LOSS\n",
    "            CE_loss_value = CE_loss(net_predictions, segmentation_classes) # XXXXXX and YYYYYYY are your inputs for the CE\n",
    "            lossTotal = CE_loss_value\n",
    "            pred = softMax(net_predictions)\n",
    "            # masks = torch.argmax(pred, dim=1)\n",
    "            # plt.imshow(masks[3])\n",
    "            # plt.colorbar()\n",
    "            # plt.show()\n",
    "\n",
    "            # DO THE STEPS FOR BACKPROP (two things to be done in pytorch)\n",
    "            lossTotal.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            confmat = ConfusionMatrix(task=\"multiclass\", num_classes=4)\n",
    "            confmat = confmat(net_predictions, segmentation_classes).numpy()\n",
    "            accuracy = np.array([confmat[0,0]/confmat[:,0].sum(),\n",
    "                        confmat[1,1]/confmat[:,1].sum(),\n",
    "                        confmat[2,2]/confmat[:,2].sum(),\n",
    "                        confmat[3,3]/confmat[:,3].sum(),]).astype(float)\n",
    "\n",
    "            accuracy[accuracy==float('nan')] = 0\n",
    "            mean_acc += accuracy\n",
    "            n += 1\n",
    "            \n",
    "            # THIS IS JUST TO VISUALIZE THE TRAINING \n",
    "            # DSCEpoch.append(computeDSC(net_predictions, segmentation_classes))\n",
    "            lossEpoch.append(lossTotal.cpu().data.numpy())\n",
    "            printProgressBar(j + 1, num_batches,\n",
    "                             prefix=\"[Training] Epoch: {} \".format(i),\n",
    "                             length=15,\n",
    "                             suffix=\" Loss: {:.4f}, Acc: [{:.4f},{:.4f},{:.4f},{:.4f}]\".format(lossTotal,accuracy[0],accuracy[1],accuracy[2],accuracy[3]))\n",
    "        \n",
    "        # DSCEpoch = np.asarray(DSCEpoch).mean()\n",
    "        # print(DSCEpoch)\n",
    "        mean_acc = mean_acc / n\n",
    "\n",
    "        lossEpoch = np.asarray(lossEpoch)\n",
    "        lossEpoch = lossEpoch.mean()\n",
    "\n",
    "        lossTotalTraining.append(lossEpoch)\n",
    "\n",
    "        \n",
    "        printProgressBar(num_batches, num_batches,\n",
    "                             done=\"[Training] Epoch: {}, LossG: {:.4f}, Acc: [{:.4f},{:.4f},{:.4f},{:.4f}]\".format(i,lossEpoch,mean_acc[0],mean_acc[1],mean_acc[2],mean_acc[3]))\n",
    "        net.eval()\n",
    "        n = 0\n",
    "        for j, data_val in enumerate(val_loader):\n",
    "            images_val, labels_val, _ = data_val\n",
    "            labels_val = to_var(labels_val)\n",
    "            images_val = to_var(labels_val)\n",
    "            \n",
    "            net_predictions_val = net.forward(images_val.float())\n",
    "\n",
    "            segmentation_classes_val = getTargetSegmentation(labels_val)\n",
    "\n",
    "            CE_loss_value_val = CE_loss(net_predictions_val, segmentation_classes_val) # XXXXXX and YYYYYYY are your inputs for the CE\n",
    "\n",
    "            confmat_val = ConfusionMatrix(task=\"multiclass\", num_classes=4)\n",
    "            confmat_val = confmat_val(net_predictions_val, segmentation_classes_val).numpy()\n",
    "            accuracy_val = np.array([confmat_val[0,0]/confmat_val[:,0].sum(),\n",
    "                            confmat_val[1,1]/confmat_val[:,1].sum(),\n",
    "                            confmat_val[2,2]/confmat_val[:,2].sum(),\n",
    "                            confmat_val[3,3]/confmat_val[:,3].sum(),]).astype(float)\n",
    "\n",
    "            accuracy_val[accuracy_val==float('nan')] = 0\n",
    "            mean_val_acc += accuracy_val\n",
    "            n += 1\n",
    "\n",
    "            lossValEpoch.append(CE_loss_value_val.cpu().data.numpy())\n",
    "            printProgressBar(j + 1, num_batches_val,\n",
    "                             prefix=\"[Validation] Epoch: {} \".format(i),\n",
    "                             length=15,\n",
    "                             suffix=\" Loss: {:.4f}, Acc: [{:.4f},{:.4f},{:.4f},{:.4f}] \".format(CE_loss_value_val,accuracy_val[0],accuracy_val[1],accuracy_val[2],accuracy_val[3]))\n",
    "        \n",
    "        mean_val_acc = mean_val_acc / n\n",
    "\n",
    "        lossValEpoch = np.asarray(lossValEpoch)\n",
    "        lossValEpoch = lossValEpoch.mean()\n",
    "\n",
    "        if lossValEpoch < Best_loss_val:\n",
    "            Best_loss_val = lossValEpoch\n",
    "            BestEpoch = i\n",
    "            if not os.path.exists('./models/' + modelName):\n",
    "                os.makedirs('./models/' + modelName)\n",
    "            torch.save({'epoch': epoch,\n",
    "                        'batch_size':batch_size,\n",
    "                        'batch_size_val':batch_size_val,\n",
    "                        'lr':lr,\n",
    "                        'model_state_dict': net.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'train_loss': lossEpoch,\n",
    "                        'val_loss': lossValEpoch,\n",
    "                        }, './models/' + modelName + '/best_model')\n",
    "            np.save(os.path.join(directory, 'Losses.npy'), lossTotalTraining)\n",
    "\n",
    "\n",
    "        printProgressBar(num_batches_val, num_batches_val,\n",
    "                             done=\"[Validation] Epoch: {}, LossG: {:.4f}, Acc: [{:.4f},{:.4f},{:.4f},{:.4f}]\".format(i,lossValEpoch,mean_val_acc[0],mean_val_acc[1],mean_val_acc[2],mean_val_acc[3]))\n",
    "        \n",
    "def LoadTraining(modelName, n_epochs):\n",
    "    if os.path.exists('./models/'+modelName+'/best_model'):\n",
    "        checkpoint = torch.load('./models/'+modelName+'/best_model')\n",
    "    else :\n",
    "        batch_size = 16\n",
    "        batch_size_val = 8\n",
    "        lr = 0.001\n",
    "        checkpoint = {\n",
    "            'epoch': 0,\n",
    "            'batch_size': batch_size,\n",
    "            'batch_size_val':batch_size_val,\n",
    "            'lr':lr,\n",
    "            'model_state_dict': None,\n",
    "            'optimizer_state_dict': None,\n",
    "            'train_loss': 0,\n",
    "            'val_loss': 1000,\n",
    "            }\n",
    "    \n",
    "    runTraining(modelName, checkpoint, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runTraining(modelName,checkpoints=None, n_epochs=0):\n",
    "    print('-' * 40)\n",
    "    print('~~~~~~~~  Starting the training... ~~~~~~')\n",
    "    print('-' * 40)\n",
    "\n",
    "    ## DEFINE HYPERPARAMETERS (batch_size > 1)\n",
    "    batch_size = 16\n",
    "    batch_size_val = 8\n",
    "    lr = 0.001\n",
    "    epoch = 10\n",
    "    start_epoch = 0\n",
    "    if checkpoints != None :\n",
    "        batch_size = checkpoints['batch_size']\n",
    "        batch_size_val = checkpoints['batch_size_val']\n",
    "        lr = checkpoints['lr']    # Learning Rate\n",
    "        epoch = n_epochs # Number of epochs\n",
    "        start_epoch = checkpoints['epoch']\n",
    "        \n",
    "\n",
    "    root_dir = './Data/'\n",
    "\n",
    "    print(' Dataset: {} '.format(root_dir))\n",
    "\n",
    "    ## DEFINE THE TRANSFORMATIONS TO DO AND THE VARIABLES FOR TRAINING AND VALIDATION\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    mask_transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    train_set_full = medicalDataLoader.MedicalImageDataset('train',\n",
    "                                                      root_dir,\n",
    "                                                      transform=transform,\n",
    "                                                      mask_transform=mask_transform,\n",
    "                                                      augment=False,\n",
    "                                                      equalize=False)\n",
    "\n",
    "    train_loader_full = DataLoader(train_set_full,\n",
    "                              batch_size=batch_size,\n",
    "                              worker_init_fn=np.random.seed(0),\n",
    "                              num_workers=0,\n",
    "                              shuffle=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    val_set = medicalDataLoader.MedicalImageDataset('val',\n",
    "                                                    root_dir,\n",
    "                                                    transform=transform,\n",
    "                                                    mask_transform=mask_transform,\n",
    "                                                    equalize=False)\n",
    "\n",
    "    val_loader = DataLoader(val_set,\n",
    "                            batch_size=batch_size_val,\n",
    "                            worker_init_fn=np.random.seed(0),\n",
    "                            num_workers=0,\n",
    "                            shuffle=False)\n",
    "\n",
    "\n",
    "    ## INITIALIZE YOUR MODEL\n",
    "    num_classes = 4 # NUMBER OF CLASSES\n",
    "\n",
    "    print(\"~~~~~~~~~~~ Creating the UNet model ~~~~~~~~~~\")\n",
    "    modelName = 'Test_Model'\n",
    "    print(\" Model Name: {}\".format(modelName))\n",
    "\n",
    "    ## CREATION OF YOUR MODEL\n",
    "    model = nnUnet(num_classes)\n",
    "\n",
    "    print(\"Total params: {0:,}\".format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "\n",
    "    # DEFINE YOUR OUTPUT COMPONENTS (e.g., SOFTMAX, LOSS FUNCTION, ETC)\n",
    "    softMax = torch.nn.Softmax()\n",
    "    CE_loss = torch.nn.CrossEntropyLoss()\n",
    "    # CE_loss = torch.nn.FocalLoss(weight=net.parameters(), ignore_index=255,\n",
    "    #                              size_average=True)\n",
    "\n",
    "    ## PUT EVERYTHING IN GPU RESOURCES    \n",
    "    if torch.cuda.is_available():\n",
    "        softMax.cuda()\n",
    "        CE_loss.cuda()\n",
    "\n",
    "    ## DEFINE YOUR OPTIMIZER\n",
    "    # optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=5e-4,momentum=0.9)\n",
    "\n",
    "    if checkpoints != None and checkpoints['optimizer_state_dict'] != None:\n",
    "        optimizer.load_state_dict = checkpoints['optimizer_state_dict']\n",
    "\n",
    "    ### To save statistics ####\n",
    "    lossTotalTraining = []\n",
    "    Best_loss_val = 1000\n",
    "    if checkpoints != None :\n",
    "        Best_loss_val = checkpoints['val_loss']\n",
    "    BestEpoch = 0\n",
    "    \n",
    "    directory = 'Results/Statistics/' + modelName\n",
    "\n",
    "    print(\"~~~~~~~~~~~ Starting the training ~~~~~~~~~~\")\n",
    "    if os.path.exists(directory)==False:\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    ## START THE TRAINING\n",
    "    \n",
    "    ## FOR EACH EPOCH\n",
    "    for i in range(start_epoch, epoch):\n",
    "        model.train()\n",
    "        lossEpoch = []\n",
    "        lossValEpoch = []\n",
    "        mean_acc = np.array([0,0,0,0]).astype(float)\n",
    "        mean_val_acc = np.array([0,0,0,0]).astype(float)\n",
    "        DSCEpoch = []\n",
    "        DSCEpoch_w = []\n",
    "        num_batches = len(train_loader_full)\n",
    "        num_batches_val = len(val_loader)\n",
    "        \n",
    "        n = 0\n",
    "        ## FOR EACH BATCH\n",
    "        for j, data in enumerate(train_loader_full):\n",
    "            ### Set to zero all the gradients\n",
    "            model.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ## GET IMAGES, LABELS and IMG NAMES\n",
    "            images, labels, _ = data\n",
    "\n",
    "            ### From numpy to torch variables\n",
    "            labels = to_var(labels)\n",
    "            # labels = torch.argmax(labels, dim=1)\n",
    "            images = to_var(images)\n",
    "\n",
    "            ################### Train ###################\n",
    "            #-- The CNN makes its predictions (forward pass)\n",
    "            net_predictions = model.forward(images)\n",
    "            #print(\"Net pred ? : \")\n",
    "            #print(net_predictions)\n",
    "            #-- Compute the losses --#\n",
    "            # THIS FUNCTION IS TO CONVERT LABELS TO A FORMAT TO BE USED IN THIS CODE\n",
    "            segmentation_classes = getTargetSegmentation(labels)\n",
    "            \n",
    "            # COMPUTE THE LOSS\n",
    "            CE_loss_value = CE_loss(net_predictions, segmentation_classes) # XXXXXX and YYYYYYY are your inputs for the CE\n",
    "            lossTotal = CE_loss_value\n",
    "            pred = softMax(net_predictions)\n",
    "            # masks = torch.argmax(pred, dim=1)\n",
    "            # plt.imshow(masks[3])\n",
    "            # plt.colorbar()\n",
    "            # plt.show()\n",
    "\n",
    "            # DO THE STEPS FOR BACKPROP (two things to be done in pytorch)\n",
    "            lossTotal.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            confmat = ConfusionMatrix(task=\"multiclass\", num_classes=4)\n",
    "            confmat = confmat(net_predictions, segmentation_classes).numpy()\n",
    "            accuracy = np.array([confmat[0,0]/confmat[:,0].sum(),\n",
    "                        confmat[1,1]/confmat[:,1].sum(),\n",
    "                        confmat[2,2]/confmat[:,2].sum(),\n",
    "                        confmat[3,3]/confmat[:,3].sum(),]).astype(float)\n",
    "\n",
    "            accuracy[accuracy==float('nan')] = 0\n",
    "            mean_acc += accuracy\n",
    "            n += 1\n",
    "            \n",
    "            # THIS IS JUST TO VISUALIZE THE TRAINING \n",
    "            # DSCEpoch.append(computeDSC(net_predictions, segmentation_classes))\n",
    "            lossEpoch.append(lossTotal.cpu().data.numpy())\n",
    "            printProgressBar(j + 1, num_batches,\n",
    "                             prefix=\"[Training] Epoch: {} \".format(i),\n",
    "                             length=15,\n",
    "                             suffix=\" Loss: {:.4f}, Acc: [{:.4f},{:.4f},{:.4f},{:.4f}]\".format(lossTotal,accuracy[0],accuracy[1],accuracy[2],accuracy[3]))\n",
    "        \n",
    "        # DSCEpoch = np.asarray(DSCEpoch).mean()\n",
    "        # print(DSCEpoch)\n",
    "        mean_acc = mean_acc / n\n",
    "\n",
    "        lossEpoch = np.asarray(lossEpoch)\n",
    "        lossEpoch = lossEpoch.mean()\n",
    "\n",
    "        lossTotalTraining.append(lossEpoch)\n",
    "\n",
    "        \n",
    "        printProgressBar(num_batches, num_batches,\n",
    "                             done=\"[Training] Epoch: {}, LossG: {:.4f}, Acc: [{:.4f},{:.4f},{:.4f},{:.4f}]\".format(i,lossEpoch,mean_acc[0],mean_acc[1],mean_acc[2],mean_acc[3]))\n",
    "        model.eval()\n",
    "        n = 0\n",
    "        for j, data_val in enumerate(val_loader):\n",
    "            images_val, labels_val, _ = data_val\n",
    "            labels_val = to_var(labels_val)\n",
    "            images_val = to_var(labels_val)\n",
    "            \n",
    "            net_predictions_val = model.forward(images_val.float())\n",
    "\n",
    "            segmentation_classes_val = getTargetSegmentation(labels_val)\n",
    "\n",
    "            CE_loss_value_val = CE_loss(net_predictions_val, segmentation_classes_val) # XXXXXX and YYYYYYY are your inputs for the CE\n",
    "\n",
    "            confmat_val = ConfusionMatrix(task=\"multiclass\", num_classes=4)\n",
    "            confmat_val = confmat_val(net_predictions_val, segmentation_classes_val).numpy()\n",
    "            accuracy_val = np.array([confmat_val[0,0]/confmat_val[:,0].sum(),\n",
    "                            confmat_val[1,1]/confmat_val[:,1].sum(),\n",
    "                            confmat_val[2,2]/confmat_val[:,2].sum(),\n",
    "                            confmat_val[3,3]/confmat_val[:,3].sum(),]).astype(float)\n",
    "\n",
    "            accuracy_val[accuracy_val==float('nan')] = 0\n",
    "            mean_val_acc += accuracy_val\n",
    "            n += 1\n",
    "\n",
    "            lossValEpoch.append(CE_loss_value_val.cpu().data.numpy())\n",
    "            printProgressBar(j + 1, num_batches_val,\n",
    "                             prefix=\"[Validation] Epoch: {} \".format(i),\n",
    "                             length=15,\n",
    "                             suffix=\" Loss: {:.4f}, Acc: [{:.4f},{:.4f},{:.4f},{:.4f}] \".format(CE_loss_value_val,accuracy_val[0],accuracy_val[1],accuracy_val[2],accuracy_val[3]))\n",
    "        \n",
    "        mean_val_acc = mean_val_acc / n\n",
    "\n",
    "        lossValEpoch = np.asarray(lossValEpoch)\n",
    "        lossValEpoch = lossValEpoch.mean()\n",
    "\n",
    "        if lossValEpoch < Best_loss_val:\n",
    "            Best_loss_val = lossValEpoch\n",
    "            BestEpoch = i\n",
    "            if not os.path.exists('./models/' + modelName):\n",
    "                os.makedirs('./models/' + modelName)\n",
    "            torch.save({'epoch': epoch,\n",
    "                        'batch_size':batch_size,\n",
    "                        'batch_size_val':batch_size_val,\n",
    "                        'lr':lr,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'train_loss': lossEpoch,\n",
    "                        'val_loss': lossValEpoch,\n",
    "                        }, './models/' + modelName + '/best_model')\n",
    "            np.save(os.path.join(directory, 'Losses.npy'), lossTotalTraining)\n",
    "\n",
    "\n",
    "        printProgressBar(num_batches_val, num_batches_val,\n",
    "                             done=\"[Validation] Epoch: {}, LossG: {:.4f}, Acc: [{:.4f},{:.4f},{:.4f},{:.4f}]\".format(i,lossValEpoch,mean_val_acc[0],mean_val_acc[1],mean_val_acc[2],mean_val_acc[3]))\n",
    "        \n",
    "def LoadTraining(modelName, n_epochs):\n",
    "    if os.path.exists('./models/'+modelName+'/best_model'):\n",
    "        checkpoint = torch.load('./models/'+modelName+'/best_model')\n",
    "    else :\n",
    "        batch_size = 16\n",
    "        batch_size_val = 8\n",
    "        lr = 0.001\n",
    "        checkpoint = {\n",
    "            'epoch': 0,\n",
    "            'batch_size': batch_size,\n",
    "            'batch_size_val':batch_size_val,\n",
    "            'lr':lr,\n",
    "            'model_state_dict': None,\n",
    "            'optimizer_state_dict': None,\n",
    "            'train_loss': 0,\n",
    "            'val_loss': 1000,\n",
    "            }\n",
    "    \n",
    "    runTraining(modelName, checkpoint, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67f81713-4c18-4dd3-bc72-35dd8f30a339",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "~~~~~~~~  Starting the training... ~~~~~~\n",
      "----------------------------------------\n",
      " Dataset: ./Data/ \n",
      "~~~~~~~~~~~ Creating the UNet model ~~~~~~~~~~\n",
      " Model Name: Test_Model\n",
      "Total params: 297,088\n",
      "~~~~~~~~~~~ Starting the training ~~~~~~~~~~\n",
      "[Training] Epoch: 0 [DONE]                                                                   \n",
      "[Training] Epoch: 0, LossG: 1.4127, Acc: [0.9651,0.0088,0.0147,0.0030]                                       \n",
      "[Validation] Epoch: 0, LossG: 1.4157, Acc: [0.0579,0.0745,nan,0.0000]                                        \n",
      "[Training] Epoch: 1 [DONE]                                                                   \n",
      "[Training] Epoch: 1, LossG: 1.3895, Acc: [0.9657,0.0079,0.0145,0.0034]                                       \n",
      "[Validation] Epoch: 1, LossG: 1.4004, Acc: [0.3849,0.1343,0.4256,0.0000]                                     \n",
      "[Training] Epoch: 2 [DONE]                                                                   \n",
      "[Training] Epoch: 2, LossG: 1.3510, Acc: [0.9789,0.0132,0.0142,0.0041]                                       \n",
      "[Validation] Epoch: 2, LossG: 1.3474, Acc: [0.8900,0.1619,0.1613,0.0000]                                     \n",
      "[Training] Epoch: 3 [DONE]                                                                   \n",
      "[Training] Epoch: 3, LossG: 1.3127, Acc: [0.9805,0.0157,0.0151,0.0079]                                       \n",
      "[Validation] Epoch: 3, LossG: 1.2800, Acc: [0.9926,0.0508,0.1573,0.0106]                                     \n",
      "[Training] Epoch: 4 [DONE]                                                                   \n",
      "[Training] Epoch: 4, LossG: 1.2618, Acc: [0.9807,0.0210,0.0143,0.0173]                                       \n",
      "[Validation] Epoch: 4, LossG: 1.1879, Acc: [0.9920,0.0518,0.1941,0.0106]                                     \n",
      "[Training] Epoch: 5 [============>  ] 84.6% Loss: 1.1891, Acc: [0.9797,0.0343,0.0216,0.0032]"
     ]
    }
   ],
   "source": [
    "runTraining('Test_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'milesial/Pytorch-UNet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Simon\\OneDrive\\ETS\\ETS2023Aut\\MTI865\\projet_segmentation\\code\\mainSegmentationChallenge.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Simon/OneDrive/ETS/ETS2023Aut/MTI865/projet_segmentation/code/mainSegmentationChallenge.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m net \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mmilesial/Pytorch-UNet\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39munet_carvana\u001b[39;49m\u001b[39m'\u001b[39;49m, pretrained\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, scale\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Simon/OneDrive/ETS/ETS2023Aut/MTI865/projet_segmentation/code/mainSegmentationChallenge.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(net)\n",
      "File \u001b[1;32mc:\\Users\\Simon\\anaconda3\\envs\\pfe\\Lib\\site-packages\\torch\\serialization.py:986\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    984\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 986\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[0;32m    987\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    988\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    989\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    990\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    991\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\Simon\\anaconda3\\envs\\pfe\\Lib\\site-packages\\torch\\serialization.py:435\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    434\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 435\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    436\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    437\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\Simon\\anaconda3\\envs\\pfe\\Lib\\site-packages\\torch\\serialization.py:416\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[1;32m--> 416\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'milesial/Pytorch-UNet'"
     ]
    }
   ],
   "source": [
    "net = torch.hub.load('milesial/Pytorch-UNet', 'unet_carvana', pretrained=True, scale=0.5)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Simon\\OneDrive\\ETS\\ETS2023Aut\\MTI865\\projet_segmentation\\code\\mainSegmentationChallenge.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Simon/OneDrive/ETS/ETS2023Aut/MTI865/projet_segmentation/code/mainSegmentationChallenge.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Simon/OneDrive/ETS/ETS2023Aut/MTI865/projet_segmentation/code/mainSegmentationChallenge.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     transforms\u001b[39m.\u001b[39mToTensor()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Simon/OneDrive/ETS/ETS2023Aut/MTI865/projet_segmentation/code/mainSegmentationChallenge.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m ])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Simon/OneDrive/ETS/ETS2023Aut/MTI865/projet_segmentation/code/mainSegmentationChallenge.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m mask_transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Simon/OneDrive/ETS/ETS2023Aut/MTI865/projet_segmentation/code/mainSegmentationChallenge.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     transforms\u001b[39m.\u001b[39mToTensor()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Simon/OneDrive/ETS/ETS2023Aut/MTI865/projet_segmentation/code/mainSegmentationChallenge.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m ])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Simon/OneDrive/ETS/ETS2023Aut/MTI865/projet_segmentation/code/mainSegmentationChallenge.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m train_set_full \u001b[39m=\u001b[39m medicalDataLoader\u001b[39m.\u001b[39mMedicalImageDataset(\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Simon/OneDrive/ETS/ETS2023Aut/MTI865/projet_segmentation/code/mainSegmentationChallenge.ipynb#W4sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                                                     \u001b[39m'\u001b[39m\u001b[39mData/\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Simon/OneDrive/ETS/ETS2023Aut/MTI865/projet_segmentation/code/mainSegmentationChallenge.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m                                                     transform\u001b[39m=\u001b[39mtransform,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Simon/OneDrive/ETS/ETS2023Aut/MTI865/projet_segmentation/code/mainSegmentationChallenge.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m                                                     mask_transform\u001b[39m=\u001b[39mmask_transform,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Simon/OneDrive/ETS/ETS2023Aut/MTI865/projet_segmentation/code/mainSegmentationChallenge.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m                                                     augment\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Simon/OneDrive/ETS/ETS2023Aut/MTI865/projet_segmentation/code/mainSegmentationChallenge.ipynb#W4sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m                                                     equalize\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'transforms' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "mask_transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_set_full = medicalDataLoader.MedicalImageDataset('train',\n",
    "                                                    'Data/',\n",
    "                                                    transform=transform,\n",
    "                                                    mask_transform=mask_transform,\n",
    "                                                    augment=False,\n",
    "                                                    equalize=False)\n",
    "\n",
    "train_loader_full = DataLoader(train_set_full,\n",
    "                            batch_size=16,\n",
    "                            worker_init_fn=np.random.seed(0),\n",
    "                            num_workers=0,\n",
    "                            shuffle=True)\n",
    "\n",
    "print(len(train_set_full))\n",
    "for item in train_set_full:\n",
    "    print(len(item))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
