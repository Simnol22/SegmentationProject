{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b07cf2f-b81d-43ef-907a-c49fed2d95ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from progressBar import printProgressBar\n",
    "\n",
    "from torchgeometry.losses import tversky\n",
    "\n",
    "import medicalDataLoader\n",
    "import argparse\n",
    "from utils import *\n",
    "\n",
    "from UNet_Base import *\n",
    "import random\n",
    "import torch\n",
    "import pdb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torchmetrics import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e976350-3fd5-4406-8511-86a06a9b4494",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        # print(inputs.shape())\n",
    "        # print(targets.shape())\n",
    "        inputs = F.sigmoid(inputs)\n",
    "\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n",
    "\n",
    "        return 1 - dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "220c7dcc-8438-454d-97b1-8e989a7b8f06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def runTraining(modelName,checkpoints=None, n_epochs=0):\n",
    "    print('-' * 40)\n",
    "    print('~~~~~~~~  Starting the training... ~~~~~~')\n",
    "    print('-' * 40)\n",
    "\n",
    "    ## DEFINE HYPERPARAMETERS (batch_size > 1)\n",
    "    batch_size = 16\n",
    "    batch_size_val = 8\n",
    "    lr = 0.001\n",
    "    epoch = 10\n",
    "    start_epoch = 0\n",
    "    if checkpoints != None :\n",
    "        batch_size = checkpoints['batch_size']\n",
    "        batch_size_val = checkpoints['batch_size_val']\n",
    "        lr = checkpoints['lr']    # Learning Rate\n",
    "        epoch = n_epochs # Number of epochs\n",
    "        start_epoch = checkpoints['epoch']\n",
    "        \n",
    "\n",
    "    root_dir = './Data/'\n",
    "\n",
    "    print(' Dataset: {} '.format(root_dir))\n",
    "\n",
    "    ## DEFINE THE TRANSFORMATIONS TO DO AND THE VARIABLES FOR TRAINING AND VALIDATION\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    mask_transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    train_set_full = medicalDataLoader.MedicalImageDataset('train',\n",
    "                                                      root_dir,\n",
    "                                                      transform=transform,\n",
    "                                                      mask_transform=mask_transform,\n",
    "                                                      augment=False,\n",
    "                                                      equalize=False)\n",
    "\n",
    "    train_loader_full = DataLoader(train_set_full,\n",
    "                              batch_size=batch_size,\n",
    "                              worker_init_fn=np.random.seed(0),\n",
    "                              num_workers=0,\n",
    "                              shuffle=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    val_set = medicalDataLoader.MedicalImageDataset('val',\n",
    "                                                    root_dir,\n",
    "                                                    transform=transform,\n",
    "                                                    mask_transform=mask_transform,\n",
    "                                                    equalize=False)\n",
    "\n",
    "    val_loader = DataLoader(val_set,\n",
    "                            batch_size=batch_size_val,\n",
    "                            worker_init_fn=np.random.seed(0),\n",
    "                            num_workers=0,\n",
    "                            shuffle=False)\n",
    "\n",
    "\n",
    "    ## INITIALIZE YOUR MODEL\n",
    "    num_classes = 4 # NUMBER OF CLASSES\n",
    "\n",
    "    print(\"~~~~~~~~~~~ Creating the UNet model ~~~~~~~~~~\")\n",
    "    modelName = 'Test_Model'\n",
    "    print(\" Model Name: {}\".format(modelName))\n",
    "\n",
    "    ## CREATION OF YOUR MODEL\n",
    "    net = UNet(num_classes)\n",
    "    if checkpoints != None and checkpoints['model_state_dict'] != None:\n",
    "        net.load_state_dict = checkpoints['model_state_dict']\n",
    "    \n",
    "\n",
    "    print(\"Total params: {0:,}\".format(sum(p.numel() for p in net.parameters() if p.requires_grad)))\n",
    "\n",
    "    # DEFINE YOUR OUTPUT COMPONENTS (e.g., SOFTMAX, LOSS FUNCTION, ETC)\n",
    "    softMax = torch.nn.Softmax()\n",
    "    CE_loss = torch.nn.CrossEntropyLoss()\n",
    "    # CE_loss = torch.nn.FocalLoss(weight=net.parameters(), ignore_index=255,\n",
    "    #                              size_average=True)\n",
    "\n",
    "    ## PUT EVERYTHING IN GPU RESOURCES    \n",
    "    if torch.cuda.is_available():\n",
    "        net.cuda()\n",
    "        softMax.cuda()\n",
    "        CE_loss.cuda()\n",
    "\n",
    "    ## DEFINE YOUR OPTIMIZER\n",
    "    # optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=5e-4,momentum=0.9)\n",
    "\n",
    "    if checkpoints != None and checkpoints['optimizer_state_dict'] != None:\n",
    "        optimizer.load_state_dict = checkpoints['optimizer_state_dict']\n",
    "\n",
    "    ### To save statistics ####\n",
    "    lossTotalTraining = []\n",
    "    Best_loss_val = 1000\n",
    "    if checkpoints != None :\n",
    "        Best_loss_val = checkpoints['val_loss']\n",
    "    BestEpoch = 0\n",
    "    \n",
    "    directory = 'Results/Statistics/' + modelName\n",
    "\n",
    "    print(\"~~~~~~~~~~~ Starting the training ~~~~~~~~~~\")\n",
    "    if os.path.exists(directory)==False:\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    ## START THE TRAINING\n",
    "    \n",
    "    ## FOR EACH EPOCH\n",
    "    for i in range(start_epoch, epoch):\n",
    "        net.train()\n",
    "        lossEpoch = []\n",
    "        lossValEpoch = []\n",
    "        mean_acc = np.array([0,0,0,0]).astype(float)\n",
    "        mean_val_acc = np.array([0,0,0,0]).astype(float)\n",
    "        DSCEpoch = []\n",
    "        DSCEpoch_w = []\n",
    "        num_batches = len(train_loader_full)\n",
    "        num_batches_val = len(val_loader)\n",
    "        \n",
    "        n = 0\n",
    "        ## FOR EACH BATCH\n",
    "        for j, data in enumerate(train_loader_full):\n",
    "            ### Set to zero all the gradients\n",
    "            net.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ## GET IMAGES, LABELS and IMG NAMES\n",
    "            images, labels, _ = data\n",
    "\n",
    "            ### From numpy to torch variables\n",
    "            labels = to_var(labels)\n",
    "            # labels = torch.argmax(labels, dim=1)\n",
    "            images = to_var(images)\n",
    "\n",
    "            ################### Train ###################\n",
    "            #-- The CNN makes its predictions (forward pass)\n",
    "            net_predictions = net.forward(images)\n",
    "\n",
    "            #-- Compute the losses --#\n",
    "            # THIS FUNCTION IS TO CONVERT LABELS TO A FORMAT TO BE USED IN THIS CODE\n",
    "            segmentation_classes = getTargetSegmentation(labels)\n",
    "            \n",
    "            # COMPUTE THE LOSS\n",
    "            CE_loss_value = CE_loss(net_predictions, segmentation_classes) # XXXXXX and YYYYYYY are your inputs for the CE\n",
    "            lossTotal = CE_loss_value\n",
    "            pred = softMax(net_predictions)\n",
    "            # masks = torch.argmax(pred, dim=1)\n",
    "            # plt.imshow(masks[3])\n",
    "            # plt.colorbar()\n",
    "            # plt.show()\n",
    "\n",
    "            # DO THE STEPS FOR BACKPROP (two things to be done in pytorch)\n",
    "            lossTotal.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            confmat = ConfusionMatrix(task=\"multiclass\", num_classes=4)\n",
    "            confmat = confmat(net_predictions, segmentation_classes).numpy()\n",
    "            accuracy = np.array([confmat[0,0]/confmat[:,0].sum(),\n",
    "                        confmat[1,1]/confmat[:,1].sum(),\n",
    "                        confmat[2,2]/confmat[:,2].sum(),\n",
    "                        confmat[3,3]/confmat[:,3].sum(),]).astype(float)\n",
    "\n",
    "            accuracy[accuracy==float('nan')] = 0\n",
    "            mean_acc += accuracy\n",
    "            n += 1\n",
    "            \n",
    "            # THIS IS JUST TO VISUALIZE THE TRAINING \n",
    "            # DSCEpoch.append(computeDSC(net_predictions, segmentation_classes))\n",
    "            lossEpoch.append(lossTotal.cpu().data.numpy())\n",
    "            printProgressBar(j + 1, num_batches,\n",
    "                             prefix=\"[Training] Epoch: {} \".format(i),\n",
    "                             length=15,\n",
    "                             suffix=\" Loss: {:.4f}, Acc: [{:.4f},{:.4f},{:.4f},{:.4f}]\".format(lossTotal,accuracy[0],accuracy[1],accuracy[2],accuracy[3]))\n",
    "        \n",
    "        # DSCEpoch = np.asarray(DSCEpoch).mean()\n",
    "        # print(DSCEpoch)\n",
    "        mean_acc = mean_acc / n\n",
    "\n",
    "        lossEpoch = np.asarray(lossEpoch)\n",
    "        lossEpoch = lossEpoch.mean()\n",
    "\n",
    "        lossTotalTraining.append(lossEpoch)\n",
    "\n",
    "        \n",
    "        printProgressBar(num_batches, num_batches,\n",
    "                             done=\"[Training] Epoch: {}, LossG: {:.4f}, Acc: [{:.4f},{:.4f},{:.4f},{:.4f}]\".format(i,lossEpoch,mean_acc[0],mean_acc[1],mean_acc[2],mean_acc[3]))\n",
    "        net.eval()\n",
    "        n = 0\n",
    "        for j, data_val in enumerate(val_loader):\n",
    "            images_val, labels_val, _ = data_val\n",
    "            labels_val = to_var(labels_val)\n",
    "            images_val = to_var(labels_val)\n",
    "            \n",
    "            net_predictions_val = net.forward(images_val.float())\n",
    "\n",
    "            segmentation_classes_val = getTargetSegmentation(labels_val)\n",
    "\n",
    "            CE_loss_value_val = CE_loss(net_predictions_val, segmentation_classes_val) # XXXXXX and YYYYYYY are your inputs for the CE\n",
    "\n",
    "            confmat_val = ConfusionMatrix(task=\"multiclass\", num_classes=4)\n",
    "            confmat_val = confmat_val(net_predictions_val, segmentation_classes_val).numpy()\n",
    "            accuracy_val = np.array([confmat_val[0,0]/confmat_val[:,0].sum(),\n",
    "                            confmat_val[1,1]/confmat_val[:,1].sum(),\n",
    "                            confmat_val[2,2]/confmat_val[:,2].sum(),\n",
    "                            confmat_val[3,3]/confmat_val[:,3].sum(),]).astype(float)\n",
    "\n",
    "            accuracy_val[accuracy_val==float('nan')] = 0\n",
    "            mean_val_acc += accuracy_val\n",
    "            n += 1\n",
    "\n",
    "            lossValEpoch.append(CE_loss_value_val.cpu().data.numpy())\n",
    "            printProgressBar(j + 1, num_batches_val,\n",
    "                             prefix=\"[Validation] Epoch: {} \".format(i),\n",
    "                             length=15,\n",
    "                             suffix=\" Loss: {:.4f}, Acc: [{:.4f},{:.4f},{:.4f},{:.4f}] \".format(CE_loss_value_val,accuracy_val[0],accuracy_val[1],accuracy_val[2],accuracy_val[3]))\n",
    "        \n",
    "        mean_val_acc = mean_val_acc / n\n",
    "\n",
    "        lossValEpoch = np.asarray(lossValEpoch)\n",
    "        lossValEpoch = lossValEpoch.mean()\n",
    "\n",
    "        if lossValEpoch < Best_loss_val:\n",
    "            Best_loss_val = lossValEpoch\n",
    "            BestEpoch = i\n",
    "            if not os.path.exists('./models/' + modelName):\n",
    "                os.makedirs('./models/' + modelName)\n",
    "            torch.save({'epoch': epoch,\n",
    "                        'batch_size':batch_size,\n",
    "                        'batch_size_val':batch_size_val,\n",
    "                        'lr':lr,\n",
    "                        'model_state_dict': net.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'train_loss': lossEpoch,\n",
    "                        'val_loss': lossValEpoch,\n",
    "                        }, './models/' + modelName + '/best_model')\n",
    "            np.save(os.path.join(directory, 'Losses.npy'), lossTotalTraining)\n",
    "\n",
    "\n",
    "        printProgressBar(num_batches_val, num_batches_val,\n",
    "                             done=\"[Validation] Epoch: {}, LossG: {:.4f}, Acc: [{:.4f},{:.4f},{:.4f},{:.4f}]\".format(i,lossValEpoch,mean_val_acc[0],mean_val_acc[1],mean_val_acc[2],mean_val_acc[3]))\n",
    "        \n",
    "def LoadTraining(modelName, n_epochs):\n",
    "    if os.path.exists('./models/'+modelName+'/best_model'):\n",
    "        checkpoint = torch.load('./models/'+modelName+'/best_model')\n",
    "    else :\n",
    "        batch_size = 16\n",
    "        batch_size_val = 8\n",
    "        lr = 0.001\n",
    "        checkpoint = {\n",
    "            'epoch': 0,\n",
    "            'batch_size': batch_size,\n",
    "            'batch_size_val':batch_size_val,\n",
    "            'lr':lr,\n",
    "            'model_state_dict': None,\n",
    "            'optimizer_state_dict': None,\n",
    "            'train_loss': 0,\n",
    "            'val_loss': 1000,\n",
    "            }\n",
    "    \n",
    "    runTraining(modelName, checkpoint, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67f81713-4c18-4dd3-bc72-35dd8f30a339",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "~~~~~~~~  Starting the training... ~~~~~~\n",
      "----------------------------------------\n",
      " Dataset: ./Data/ \n",
      "~~~~~~~~~~~ Creating the UNet model ~~~~~~~~~~\n",
      " Model Name: Test_Model\n",
      "Total params: 60,664\n",
      "~~~~~~~~~~~ Starting the training ~~~~~~~~~~\n",
      "[Training] Epoch: 0 [DONE]                                                                   \n",
      "[Training] Epoch: 0, LossG: 1.8645, Acc: [0.9291,0.0082,0.0088,0.0110]                                       \n",
      "[Validation] Epoch: 0 [DONE]                                                                    \n",
      "[Validation] Epoch: 0, LossG: 1.6057, Acc: [0.1078,0.2711,0.0021,0.0000]                                     \n",
      "[Training] Epoch: 1 [DONE]                                                                   \n",
      "[Training] Epoch: 1, LossG: 1.5915, Acc: [0.9329,0.0066,0.0070,0.0105]                                       \n",
      "[Validation] Epoch: 1 [DONE]                                                                    \n",
      "[Validation] Epoch: 1, LossG: 1.5172, Acc: [0.1889,0.1954,0.0018,0.0011]                                     \n",
      "[Training] Epoch: 2 [DONE]                                                                   \n",
      "[Training] Epoch: 2, LossG: 1.3061, Acc: [0.9435,0.0050,0.0044,0.0107]                                       \n",
      "[Validation] Epoch: 2 [DONE]                                                                    \n",
      "[Validation] Epoch: 2, LossG: 1.3141, Acc: [0.9366,0.0858,0.0014,0.0216]                                     \n",
      "[Training] Epoch: 3 [======>        ] 46.2% Loss: 1.0980, Acc: [0.9658,0.0064,0.0039,0.0183]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\79rap\\Documents\\GitHub\\projet_segmentation\\code\\mainSegmentationChallenge.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/79rap/Documents/GitHub/projet_segmentation/code/mainSegmentationChallenge.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m runTraining(\u001b[39m'\u001b[39;49m\u001b[39mTest_model\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\79rap\\Documents\\GitHub\\projet_segmentation\\code\\mainSegmentationChallenge.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/79rap/Documents/GitHub/projet_segmentation/code/mainSegmentationChallenge.ipynb#W3sZmlsZQ%3D%3D?line=150'>151</a>\u001b[0m pred \u001b[39m=\u001b[39m softMax(net_predictions)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/79rap/Documents/GitHub/projet_segmentation/code/mainSegmentationChallenge.ipynb#W3sZmlsZQ%3D%3D?line=151'>152</a>\u001b[0m \u001b[39m# masks = torch.argmax(pred, dim=1)\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/79rap/Documents/GitHub/projet_segmentation/code/mainSegmentationChallenge.ipynb#W3sZmlsZQ%3D%3D?line=152'>153</a>\u001b[0m \u001b[39m# plt.imshow(masks[3])\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/79rap/Documents/GitHub/projet_segmentation/code/mainSegmentationChallenge.ipynb#W3sZmlsZQ%3D%3D?line=153'>154</a>\u001b[0m \u001b[39m# plt.colorbar()\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/79rap/Documents/GitHub/projet_segmentation/code/mainSegmentationChallenge.ipynb#W3sZmlsZQ%3D%3D?line=154'>155</a>\u001b[0m \u001b[39m# plt.show()\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/79rap/Documents/GitHub/projet_segmentation/code/mainSegmentationChallenge.ipynb#W3sZmlsZQ%3D%3D?line=155'>156</a>\u001b[0m \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/79rap/Documents/GitHub/projet_segmentation/code/mainSegmentationChallenge.ipynb#W3sZmlsZQ%3D%3D?line=156'>157</a>\u001b[0m \u001b[39m# DO THE STEPS FOR BACKPROP (two things to be done in pytorch)\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/79rap/Documents/GitHub/projet_segmentation/code/mainSegmentationChallenge.ipynb#W3sZmlsZQ%3D%3D?line=157'>158</a>\u001b[0m lossTotal\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/79rap/Documents/GitHub/projet_segmentation/code/mainSegmentationChallenge.ipynb#W3sZmlsZQ%3D%3D?line=158'>159</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/79rap/Documents/GitHub/projet_segmentation/code/mainSegmentationChallenge.ipynb#W3sZmlsZQ%3D%3D?line=160'>161</a>\u001b[0m confmat \u001b[39m=\u001b[39m ConfusionMatrix(task\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m, num_classes\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    259\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "runTraining('Test_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\79rap\\Documents\\GitHub\\projet_segmentation\\code\\mainSegmentationChallenge.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/79rap/Documents/GitHub/projet_segmentation/code/mainSegmentationChallenge.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/79rap/Documents/GitHub/projet_segmentation/code/mainSegmentationChallenge.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     transforms\u001b[39m.\u001b[39mToTensor()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/79rap/Documents/GitHub/projet_segmentation/code/mainSegmentationChallenge.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m ])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/79rap/Documents/GitHub/projet_segmentation/code/mainSegmentationChallenge.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m mask_transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/79rap/Documents/GitHub/projet_segmentation/code/mainSegmentationChallenge.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     transforms\u001b[39m.\u001b[39mToTensor()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/79rap/Documents/GitHub/projet_segmentation/code/mainSegmentationChallenge.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m ])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/79rap/Documents/GitHub/projet_segmentation/code/mainSegmentationChallenge.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m train_set_full \u001b[39m=\u001b[39m medicalDataLoader\u001b[39m.\u001b[39mMedicalImageDataset(\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/79rap/Documents/GitHub/projet_segmentation/code/mainSegmentationChallenge.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                                                     \u001b[39m'\u001b[39m\u001b[39mData/\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/79rap/Documents/GitHub/projet_segmentation/code/mainSegmentationChallenge.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m                                                     transform\u001b[39m=\u001b[39mtransform,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/79rap/Documents/GitHub/projet_segmentation/code/mainSegmentationChallenge.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m                                                     mask_transform\u001b[39m=\u001b[39mmask_transform,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/79rap/Documents/GitHub/projet_segmentation/code/mainSegmentationChallenge.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m                                                     augment\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/79rap/Documents/GitHub/projet_segmentation/code/mainSegmentationChallenge.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m                                                     equalize\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'transforms' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "mask_transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_set_full = medicalDataLoader.MedicalImageDataset('train',\n",
    "                                                    'Data/',\n",
    "                                                    transform=transform,\n",
    "                                                    mask_transform=mask_transform,\n",
    "                                                    augment=False,\n",
    "                                                    equalize=False)\n",
    "\n",
    "train_loader_full = DataLoader(train_set_full,\n",
    "                            batch_size=16,\n",
    "                            worker_init_fn=np.random.seed(0),\n",
    "                            num_workers=0,\n",
    "                            shuffle=True)\n",
    "\n",
    "print(len(train_set_full))\n",
    "for item in train_set_full:\n",
    "    print(len(item))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
